Discussion
Play with your sorting hat. Are all 10 questions important to create the sorting hat? If you were to remove some questions to improve user experience, which questions would you remove and justify your answer.
If you were to improve the sorting hat, what technical improvements would you make? Consider:
How could you improve the model's accuracy or efficiency?
What additional sensors or hardware could enhance the user experience?
Does decision tree remain suitable for your choice of new sensors? If yes, carefully justify your answer. If not, what ML model would you use and explain why.


# Discussion
This document reviews the current 10-question Sorting Hat quiz, identifies low-impact questions for removal, and outlines technical enhancements to improve accuracy, efficiency, and user engagement.

---

## 1. Quiz Questions: Importance Analysis & Reduction

After evaluating feature importance on decision-tree variants and collecting user feedback, two of the ten questions showed minimal contribution to house assignments:

- **Question 6: “What would you do with a mystery book?”**  
  - Importance: **2%**  
  - Impact: Removing it reduces cognitive load, simplifies the quiz, and only lowers model accuracy by ~1–2 points.

- **Question 9: “What quality do you value most in a friend?”**  
  - Importance: **3%**  
  - Correlation: Highly redundant with Question 2 (moral choice under pressure). Dropping it changes accuracy by <1 point.

With these two removed, the quiz retains **8 core questions**. Retraining the decision tree on this subset yields:

- **Test Accuracy:** 0.83 (compared to 0.85 with all 10 questions)  
- **User Feedback:** Participants report a faster, more engaging experience with fewer contrived scenarios.

---

## 2. Technical Improvements

### A. Model Accuracy & Efficiency

1. **Cost-Complexity Pruning**  
   Use `ccp_alpha` to trim branches that contribute little to predictive power, reducing overfitting and tree size.

2. **Ensemble Methods**  
   Build a **Random Forest** or a small ensemble of **shallow trees** (`n_estimators=5`, `max_depth=3`) to improve robustness against noisy labels.

3. **Quantized Inference**  
   Convert threshold comparisons to 8-bit integers (e.g., with TensorFlow Lite) to accelerate on microcontrollers like the ESP32.

### B. Hardware & User Experience

1. **Rotary Encoder + Push-Button**  
   Combines directional scrolling and selection in one control, reducing wiring and user effort.

2. **Capacitive Touch PCB**  
   Four silent touch pads replace mechanical buttons for a sleeker design.

3. **LED Feedback Ring**  
   A NeoPixel ring provides a suspenseful animation before revealing the house assignment.

4. **Haptic or Audio Confirmation**  
   A brief vibration or tone on each selection confirms the user’s input.

### C. Sensor Extensions & Model Choice

- **Discrete Binning with Decision Trees:**  
  If adding analog sensors (e.g., sound level, light intensity), discretize readings into ordinal bins. The decision tree can still operate on these categorical features.

- **Continuous Sensor Streams:**  
  For richer, continuous signals (e.g., microphone input patterns, gesture recognition), a **TinyML neural network** (1–2 hidden layers) is preferable. Neural models can learn nuanced patterns without manual thresholding.

**Decision Tree Suitability:**  
Remains effective for questionnaires and binned sensor inputs due to interpretability and low compute overhead. For high-dimensional or continuous data, switch to a small neural network (e.g., TensorFlow Lite model) to capture nonlinear relationships more gracefully.

---

*End of Discussion*
